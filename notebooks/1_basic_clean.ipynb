{"cells":[{"cell_type":"code","source":["# Example notebook code from:\n# https://docs.microsoft.com/en-us/azure/databricks/_static/notebooks/getting-started/popvspricelr.html\n# https://docs.microsoft.com/en-us/azure/databricks/getting-started/spark/machine-learning\n\nimport azureml.core\nfrom azureml.core.authentication import ServicePrincipalAuthentication\nfrom azureml.core import Workspace, Dataset\n\nprint(\"SDK version:\", azureml.core.VERSION)\n\nkeyVaultScope = \"databricks-aml-demo\"\n\nservice_principal_id = dbutils.secrets.get(keyVaultScope, \"databricks-aml-demo-sp-client-id\")\nservice_principal_password = dbutils.secrets.get(keyVaultScope, \"databricks-aml-demo-sp-client-key\")\ntenant_id = dbutils.secrets.get(keyVaultScope, \"azure-tenant-id\")\n\n# AML Workspace\nworkspace_name = \"amls-databricks\"\nsubscription_id = dbutils.secrets.get(keyVaultScope, \"azure-subscription-id\")\nresource_group = \"jp-databricks\"\n\nsvc_pr = ServicePrincipalAuthentication(\n    service_principal_id=service_principal_id,\n    service_principal_password=service_principal_password,\n    tenant_id=tenant_id\n    )\n\nws = Workspace(workspace_name=workspace_name,\n               subscription_id=subscription_id,\n               resource_group=resource_group,\n               auth=svc_pr)\n\nprint(\"Found workspace {} at location {}\".format(ws.name, ws.location))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">SDK version: 1.3.0\nFound workspace amls-databricks at location centralus\n</div>"]}}],"execution_count":1},{"cell_type":"code","source":["# Get default datastore\ndatastore = ws.get_default_datastore()\n\ndata_geo = Dataset.get_by_name(ws, name='data_geo')\ndata = data_geo.to_spark_dataframe()\ndata.cache()\n\n# display(data)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["# Data cleaning\n# Drop rows with missing values, replace column headings\nfrom pyspark.sql.functions import col\n\nexprs = [col(column).alias(column.replace(' ', '_')) for column in data.columns]\ndata = data.dropna().select(*exprs) "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["# Write output to datastore\ndata.write.parquet(f\"/mnt/{datastore.container_name}/population-vs-price/data-geo-prepped/\", mode=\"overwrite\")"],"metadata":{},"outputs":[],"execution_count":4}],"metadata":{"name":"1_basic_clean","notebookId":2864793919988463},"nbformat":4,"nbformat_minor":0}
